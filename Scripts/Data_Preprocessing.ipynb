{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddN_bdlkjkAb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentAnalyzer\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.sentiment.util import *\n",
        "from nltk import tokenize\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tag import PerceptronTagger\n",
        "from nltk.data import find\n",
        "from nltk.stem import *\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4wRM2Q0jtwY"
      },
      "outputs": [],
      "source": [
        "year = '2013'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6A9GRXSjq9K"
      },
      "outputs": [],
      "source": [
        "df_lst = []\n",
        "file_path = f\"/content/drive/MyDrive/Uni stuff/Classes/Fourth Year MI/Thesis/Colab/{year}/\"\n",
        "#os.rmdir(file_path + \".ipynb_checkpoints\")\n",
        "#os.remove(file_path + \".\")\n",
        "\n",
        "i = 0\n",
        "for folder in os.listdir(file_path):\n",
        "    for f in os.listdir(file_path + folder + '/'):\n",
        "        df_lst += [pd.read_csv(file_path + folder + '/' + f)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOzMy9kAlkYg"
      },
      "outputs": [],
      "source": [
        "full_df = pd.concat(df_lst)\n",
        "full_df[\"speakerparty\"] = full_df[\"speakerparty\"].replace(\"NDP\", \"New Democratic Party\")\n",
        "full_df[\"speakerparty\"] = full_df[\"speakerparty\"].replace(\"Bloc\", \"Bloc Québécois\")\n",
        "full_df[\"speakerparty\"] = full_df[\"speakerparty\"].replace(\"Green\", \"Green Party\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "full_df.head(2)"
      ],
      "metadata": {
        "id": "pPnbAna_-87A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMLriIOjlzyp"
      },
      "outputs": [],
      "source": [
        "reduced_df = full_df[[\"speakername\", \"speakerparty\", \"speechtext\"]]\n",
        "reduced_df.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9IwiI0wCDrq"
      },
      "source": [
        "## Web Scraping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIXptDILHNV4"
      },
      "source": [
        "### Cities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXPnhJ_XCyW6"
      },
      "outputs": [],
      "source": [
        "import requests # library to handle requests\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3qh5DfdDDuD"
      },
      "outputs": [],
      "source": [
        "# get the response in the form of html\n",
        "wikiurl=\"https://en.wikipedia.org/wiki/List_of_the_largest_population_centres_in_Canada\"\n",
        "table_class=\"wikitable sortable jquery-tablesorter\"\n",
        "response=requests.get(wikiurl)\n",
        "print(response.status_code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9cAFTfuDELL"
      },
      "outputs": [],
      "source": [
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "indiatable=soup.find('table',{'class':\"wikitable\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8GwqzWYDEpR"
      },
      "outputs": [],
      "source": [
        "city_df=pd.read_html(str(indiatable))\n",
        "# convert list to dataframe\n",
        "city_df=pd.DataFrame(city_df[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2Om42w7FLfW"
      },
      "outputs": [],
      "source": [
        "cities = city_df['Population centre[5]'].tolist()\n",
        "cities += [\"Ottawa\", \"Gatineau\", \"Niagara Falls\", \"St. Catharines\", \"Sydney\", \"Whitehorse\", \"Yellowknife\", \"Iqaluit\"]\n",
        "cities = [city.lower() for city in cities]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qq1itk2gHQJJ"
      },
      "source": [
        "### Names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6K38n6ZYPYX8"
      },
      "outputs": [],
      "source": [
        "names = pd.read_csv('https://www.usna.edu/Users/cs/roche/courses/s15si335/proj1/files.php%3Ff=names.txt&downloadcode=yes', skiprows=0, header=None, sep='\\s+')[0].to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqTqAdQ5HSk2"
      },
      "outputs": [],
      "source": [
        "names += [item for sublist in reduced_df[\"speakername\"].str.split(\" \").to_list() for item in sublist]\n",
        "names = [item.lower() for item in names]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwMA6rGM_rx0"
      },
      "source": [
        "## Tokenization, Stemming, Stop Words and Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SsWQI_VuNeob"
      },
      "outputs": [],
      "source": [
        "tokenizer = RegexpTokenizer(r'\\w+')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0liqH8Fa_xKy"
      },
      "outputs": [],
      "source": [
        "reduced_df[\"tokenized\"] = reduced_df[\"speechtext\"].str.lower().apply(tokenizer.tokenize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHfXVGMxBCG-"
      },
      "outputs": [],
      "source": [
        "stop = set(stopwords.words('english'))\n",
        "stop.add(\"mr\")\n",
        "stop.add(\"speaker\")\n",
        "stop.update(set(names))\n",
        "stop.update(set(cities))\n",
        "\n",
        "stemmer = PorterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKm6PvS-BKPU"
      },
      "outputs": [],
      "source": [
        "reduced_df[\"stem_no_stopwords\"] = reduced_df[\"tokenized\"].apply(lambda x: [stemmer.stem(item) for item in x if item not in stop])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFAXSQBAB5rX"
      },
      "outputs": [],
      "source": [
        "full_stem_list = [a for b in reduced_df[\"stem_no_stopwords\"].tolist() for a in b]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfkH3hucB8hf"
      },
      "outputs": [],
      "source": [
        "num_tokens = len(full_stem_list)\n",
        "\n",
        "unique_tokens = set(full_stem_list)\n",
        "num_unique_tokens = len(unique_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nEGVZbRIgpo"
      },
      "outputs": [],
      "source": [
        "i = 0\n",
        "i_max_achieved = 0\n",
        "rm = 0\n",
        "full_stem_list_reduced = full_stem_list\n",
        "\n",
        "####\n",
        "i_max_achieved = 15000\n",
        "full_stem_list_reduced = pd.read_csv(f'/content/drive/MyDrive/Uni stuff/Classes/Fourth Year MI/Thesis/reduced_full_stem_after_{i_max_achieved}_words.csv')[\"words\"].to_list()\n",
        "####\n",
        "\n",
        "for token in unique_tokens:\n",
        "    i += 1\n",
        "    if i < i_max_achieved:\n",
        "        continue\n",
        "\n",
        "    if full_stem_list.count(token) < 50:\n",
        "        full_stem_list_reduced = list(filter(lambda a: a != token, full_stem_list_reduced))\n",
        "        rm += 1\n",
        "    if i % 500 == 0:\n",
        "        print(\"{} completed\".format(i))\n",
        "    if i % 5000 == 0:\n",
        "        save_df = pd.DataFrame(full_stem_list_reduced, columns=[\"words\"])\n",
        "        save_df.to_csv(f'/content/drive/MyDrive/Uni stuff/Classes/Fourth Year MI/Thesis/Checkpoints/reduced_full_stem_after_{i}_words.csv', index=False)\n",
        "\n",
        "print(\"Total removed: \" + str(rm))\n",
        "save_df.to_csv(f'/content/drive/MyDrive/Uni stuff/Classes/Fourth Year MI/Thesis/Checkpoints/reduced_full_stem_done_{year}.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XBAIkwRP2XL"
      },
      "outputs": [],
      "source": [
        "reduced_unique_tokens = set(full_stem_list_reduced)\n",
        "reduced_unique_tokens = {x for x in reduced_unique_tokens if x==x} # removes nan\n",
        "\n",
        "full_stem_list_reduced_common = full_stem_list_reduced\n",
        "half_records = len(reduced_df[\"stem_no_stopwords\"])//2\n",
        "\n",
        "i = 0\n",
        "rm = 0\n",
        "\n",
        "for token in reduced_unique_tokens:\n",
        "    i += 1\n",
        "    if sum(token in item for item in reduced_df[\"stem_no_stopwords\"].tolist()) > half_records:\n",
        "        full_stem_list_reduced_common = list(filter(lambda a: a != token, full_stem_list_reduced_common))\n",
        "        rm += 1\n",
        "    if token[0].isdigit() or (token[0] == '-' and token[1:].isdigit()):\n",
        "        full_stem_list_reduced_common = list(filter(lambda a: a != token, full_stem_list_reduced_common))\n",
        "        rm += 1\n",
        "    \n",
        "    if i % 500 == 0:\n",
        "        print(\"{} completed\".format(i))\n",
        "    if i % 5000 == 0:\n",
        "        save_df = pd.DataFrame(full_stem_list_reduced, columns=[\"words\"])\n",
        "        save_df.to_csv(f'/content/drive/MyDrive/Uni stuff/Classes/Fourth Year MI/Thesis/Checkpoints/common_reduced_full_stem_after_{i}_words.csv', index=False)\n",
        "\n",
        "print(\"Total removed: \" + str(rm))\n",
        "save_df.to_csv(f'/content/drive/MyDrive/Uni stuff/Classes/Fourth Year MI/Thesis/Checkpoints/common_reduced_full_stem_done_{year}.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ji02Sc2MnfNw"
      },
      "outputs": [],
      "source": [
        "reduced_unique_tokens_common = set(full_stem_list_reduced_common)\n",
        "\n",
        "reduced_df[\"processed\"] = \"\"\n",
        "\n",
        "for i, row in enumerate(reduced_df[\"stem_no_stopwords\"].tolist()):\n",
        "    reduced_df[\"processed\"].iloc[i] = list(filter(lambda a: a not in reduced_unique_tokens_common, row))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fyd__CGMcC8O"
      },
      "outputs": [],
      "source": [
        "reduced_df = reduced_df[reduced_df[\"processed\"].apply(lambda x: len(x) > 4)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQ8x82Q-m-Io"
      },
      "outputs": [],
      "source": [
        "reduced_df.to_csv(f'/content/drive/MyDrive/Uni stuff/Classes/Fourth Year MI/Thesis/Processed Data/processed_{year}.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "G9IwiI0wCDrq",
        "LIXptDILHNV4",
        "Qq1itk2gHQJJ"
      ],
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}